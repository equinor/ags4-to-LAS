{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef31f46f",
   "metadata": {},
   "source": [
    "# Convert AGS4 files to LAS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "262b6435",
   "metadata": {},
   "source": [
    "### Load libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ae19e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c25354c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import os\n",
    "from python_ags4 import AGS4\n",
    "import lasio\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f61bcab9",
   "metadata": {},
   "source": [
    "### Function to scan folder (poss. recursively) for files with a specific extension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7582b0ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_files(path, recursive=False, extension='', case_sensitive=False):\n",
    "    '''Function to scan a path (optional recursive) to find all files with a specific\n",
    "    extension.\n",
    "    Mandatory argument:\n",
    "    :path: path/directory to look in for files \n",
    "    Optional arguments:\n",
    "    :recursive: decends into directories. Default = False\n",
    "    :extension: file extension. Default = ''\n",
    "    :case_sensitive. Treats extention as case sensitive. Default = False\n",
    "    Returns a list with all files matching the extension. Files/elements contain the full path.'''\n",
    "    file_list = list()\n",
    "    \n",
    "    for f in os.listdir(path):\n",
    "        fpath = os.path.join(path,f)\n",
    "        if os.path.isdir(fpath):\n",
    "            if recursive == True:\n",
    "                file_list.extend(find_files(fpath, recursive=recursive, extension=extension, case_sensitive=case_sensitive))\n",
    "        else:\n",
    "            if case_sensitive == True:\n",
    "                if fpath.endswith(extension):\n",
    "                    file_list.append(fpath)\n",
    "            else:\n",
    "                if fpath.lower().endswith(extension.lower()):\n",
    "                    file_list.append(fpath)\n",
    "    return(file_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b6b2e94",
   "metadata": {},
   "source": [
    "### Only the following tables (GROUPS) and columns (HEADERS) will be extracted from the AGS file for each location (if present). (this can be changed later, but for starters only the curves that are present in typical plots were extracted)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb52a6ef",
   "metadata": {},
   "source": [
    "#### The following codes (depth/variable pairs) need to be collected for the LAS file:\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<span style=\"font-family:Courier New\">\n",
    "- SCPT -- SCPT_RES:  qc<br>\n",
    "- &nbsp;&nbsp;&nbsp;&nbsp; -- SCPT_FRES: fs<br>\n",
    "- &nbsp;&nbsp;&nbsp;&nbsp; -- SCPT_QT:   qt<br>\n",
    "- &nbsp;&nbsp;&nbsp;&nbsp; -- SCPT_NQT:  Qt<br>\n",
    "- &nbsp;&nbsp;&nbsp;&nbsp; -- SCPT_BQ:   Bq<br>\n",
    "- &nbsp;&nbsp;&nbsp;&nbsp; -- SCPT_NFR:  Fr<br>\n",
    "- SCDT -- SCDT_PWP2: u2<br>\n",
    "- SCPP -- SCPP_CIC:  Ic(n)<br>\n",
    "</span>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c16a90",
   "metadata": {},
   "source": [
    "Create a list with these codes (note that the GROUP/table names appear as prefix in column headers. Use the column headers as code in the list - e.g. \"SCPT_RES\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71060489",
   "metadata": {},
   "outputs": [],
   "source": [
    "codes = ['SCPT_RES', 'SCPT_FRES', 'SCPT_PWP2', 'SCDT_PWP2', 'SCPT_QT', 'SCPT_NQT', 'SCPT_BQ', 'SCPT_NFR', 'SCPP_CIC']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2483ceb4",
   "metadata": {},
   "source": [
    "Create a list with code that will be used to fill the \"PARAMETER\" section in the LAS file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "913a3888",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = ['LOCA_NATN','LOCA_NATE', 'LOCA_GL', 'LOCA_FDEP', 'LOCA_WDEP', 'LOCA_DATM', 'LOCA_LAT', 'LOCA_LON']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a7e02f",
   "metadata": {},
   "source": [
    "#### Some other user settings: all tables with depth-related data in AGS have a column with suffix \"_DPTH\". However, we only need one in the final LAS file: col_depth=..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539699b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "depth_suffix = '_DPTH'\n",
    "depth_col = 'DEPTH'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef180bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "group_headings_file = '../background/group_headings_dict_file.csv'\n",
    "group_headings = pd.read_csv(group_headings_file, sep='|', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e8396d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_group_headings_dictionary(group_headings=group_headings):\n",
    "    '''Creates a dictionaty with GROUP (=names of tables in the AGS file)\n",
    "    a dictionary as value. The latter dictionary contains column names (=keys)\n",
    "    within the GROUP and description of the columns as values.\n",
    "    Returns a (nested) dictionary. First level: GROUP = key, value is (second level)\n",
    "      dictionary of all columns-headers in GROUP (the keys in the nested dictionary),\n",
    "      with the description of the column-headers as values.'''\n",
    "    group_headings_dict = dict()\n",
    "    for g in group_headings.index.unique():\n",
    "        tmp_dict = dict()\n",
    "        for h in group_headings.loc[group_headings.index==g, 'Heading'].unique():\n",
    "            try:\n",
    "                tmp_dict[h]=group_headings.loc[(group_headings.index==g) & (group_headings['Heading']==h),'Description'].values[0]\n",
    "            except:\n",
    "                pass\n",
    "        group_headings_dict[g]= tmp_dict\n",
    "    return group_headings_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c5c1aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "group_headings_dict = create_group_headings_dictionary(group_headings=group_headings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10992ad0",
   "metadata": {},
   "source": [
    "Convert to dictinaty ({code: description})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a95650e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "codes = dict([(c, group_headings_dict[c[:4]][c]) for c in codes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1772c042",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = dict([(p, group_headings_dict[p[:4]].get(p,p)) for p in parameters])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff32ff3",
   "metadata": {},
   "source": [
    "#### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaeb454a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_data_for_single_location(loca, file, tables):\n",
    "    '''Creates a LAS file for data picked up for a one location\n",
    "    Mandatory input:\n",
    "    :loca: unique location name, used in the AGS file\n",
    "    :file: name of the AGS4 file (used to add as meta-data to the LAS file and in the logging)\n",
    "    Returns one LAS file per location with \"{loca}_({ags4_file}).LAS\" as filename. Besides, this\n",
    "    function returns a code (0, 1, or 2 for OK, WARNING, resp ERROR) for each location/file > LAS\n",
    "    combination. This info is written to a log in the LAS export location for later reference.'''\n",
    "    try:\n",
    "        process_code = 0\n",
    "        _dfs_list = pick_up_depth_related_data(loca, codes)\n",
    "        df = check_whether_different_dataframes_depth_related_data_are_on_same_depths(_dfs_list)\n",
    "        las = create_and_fill_LAS_file(df, tables, loca, codes, parameters, file)\n",
    "        filename = loca + '__' + file.split(os.sep)[-1] + '__.LAS'\n",
    "        las.write(os.path.join(export_folder,filename), version=2)\n",
    "        print(f'...[OK!] LAS file created for {loca} in {file.split(os.sep)[-1]}...\\n')\n",
    "    except:\n",
    "        if len(_dfs_list)==0 or _dfs_list==None:\n",
    "            process_code = 1\n",
    "            print(f'...[WARNING] no logs found for {loca} in {file.split(os.sep)[-1]}...\\n')\n",
    "        else:\n",
    "            process_code = 2\n",
    "            print(f'...[ERROR] with {loca} in {file.split(os.sep)[-1]}:...\\n')\n",
    "    return(process_code, loca, file.split(os.sep)[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2483fcdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pick_up_depth_related_data(loca, codes):\n",
    "    '''Picks up all depth related data\n",
    "    Mandatory input:\n",
    "    :loca: unique location name, used in the AGS file\n",
    "    :codes: dictionary with all (depth-related) codes that should be looked for. The keys should\n",
    "       be the GROUP_HEADER codes (e.g. \"SCPT_QT\") that will be looked up; the values are\n",
    "       the descriptions\n",
    "    Returns a list with each element in the list being one depth-related curve from the \"codes dictionary\"'''\n",
    "    _dfs_list = list()\n",
    "    groups = list()\n",
    "    \n",
    "    for code in codes.keys():\n",
    "        groups.append(code.split('_')[0])\n",
    "    groups = list(set(groups))\n",
    "\n",
    "    for group in groups:\n",
    "        try:\n",
    "            cols = [c for c in tables[group].loc[(tables[group]['LOCA_ID']==loca)].columns if c in [c for c in codes.keys()]]\n",
    "            depth_col = group+depth_suffix\n",
    "            _df = tables[group].loc[(tables[group]['LOCA_ID']==loca), [group+depth_suffix]+cols]\n",
    "            if len(_df)>0:\n",
    "                _dfs_list.append(_df)\n",
    "        except:\n",
    "            pass\n",
    "    return _dfs_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3aa3c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_whether_different_dataframes_depth_related_data_are_on_same_depths(_dfs_list):\n",
    "    '''Checks whether different dataframes are on the same depth grid.\n",
    "    Mandatory input:\n",
    "    :_dfs_list: list with each element being a dataframe (obtained from function \"pick_up_depth_related_data\")\n",
    "    Returns a pandas DataFrame (product from concatenating all elements along the column axis)'''\n",
    "    # use first dataframe in \"collector\" as standard to compare depths against\n",
    "    if len(_dfs_list)>0:\n",
    "        # make numeric first (all numbers are strings in AGS4 files?)\n",
    "        for c in _dfs_list[0].columns:\n",
    "            _dfs_list[0][c] = pd.to_numeric(_dfs_list[0][c], errors='coerce')\n",
    "            \n",
    "        for c in _dfs_list[0].columns:\n",
    "            if c.endswith(depth_suffix):\n",
    "                _df0 = _dfs_list[0].copy(deep=True)\n",
    "                _df0.rename(columns={c:depth_col}, inplace=True)\n",
    "                \n",
    "    # run through all other dataframes in \"collector\":\n",
    "    for d in range(1,len(_dfs_list)):\n",
    "        # make numeric first (all numbers are strings in AGS4 files?)            \n",
    "        _df1 = _dfs_list[d].copy(deep=True)\n",
    "        for c in _df1.columns:\n",
    "            _df1[c] = pd.to_numeric(_df1[c], errors='coerce')\n",
    "\n",
    "        for c in _df1.columns:\n",
    "            if c.endswith(depth_suffix):\n",
    "                # but rename first to avoid getting it nevertheless\n",
    "                _df1.rename(columns={c:depth_col}, inplace=True)\n",
    "                if _df0[depth_col].equals(_df1[depth_col]):\n",
    "                    # same depths: merge \n",
    "                    _df0 = pd.merge(_df0, _df1, how='left', on=depth_col)\n",
    "                else:\n",
    "                    # IF DATA IS NOT ON THE SAME \"DEPTH GRID\", THIS IS NOT HANDLED CURRENTLY\n",
    "                    pass\n",
    "    return _df0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43576ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_and_fill_LAS_file(df, tables, loca, codes, parameters, file):\n",
    "    '''Wrapper function containing all functions creating the different sections of the LAS file.\n",
    "    Mandatory input:\n",
    "    :df: pandas DataFrame containing the depth-related data (the DataFrame is generated through \n",
    "       functions \"pick_up_depth_related_data\" and\n",
    "       \"check_whether_different_dataframes_depth_related_data_are_on_same_depths\")\n",
    "    :tables: tables-section from AGS file\n",
    "    :codes: dictionary with all (depth-related) codes that should be looked for. The keys should\n",
    "       be the GROUP_HEADER codes (e.g. \"SCPT_QT\") that will be looked up; the values are the \n",
    "       descriptions\n",
    "    :parameters: dictionary with codes with meta-data that will eventually go into the\n",
    "       parameters section of the LAS file\n",
    "    :loca: unique location name, used in the AGS file\n",
    "    :file: name of the AGS4 file (used to add as meta-data to the LAS file and in the logging)\n",
    "    Returns the las-object'''\n",
    "    las = lasio.LASFile()\n",
    "    fill_LAS_well_section(las, df, tables, loca)\n",
    "    fill_LAS_curve_section(las, df, codes)\n",
    "    fill_LAS_params_section(las, tables, loca, parameters)\n",
    "    fill_LAS_other_section(las, loca, file)\n",
    "    return las"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "054815c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sampling_rate(df, depth_col):\n",
    "    '''Function to get the sampling rate of the passed DataFrame.\n",
    "    Mandatory arguments:\n",
    "    :df: pandas DataFrame with concatenated depth-related data (the DataFrame is generated through \n",
    "       functions \"pick_up_depth_related_data\" and\n",
    "       \"check_whether_different_dataframes_depth_related_data_are_on_same_depths\")\n",
    "    :depth_col: name of the (renamed) depth column\n",
    "    Returns the sampling rate (used for the LAS header)'''\n",
    "    try:\n",
    "        return(round(df[depth_col].diff().median(), 4))\n",
    "    except:\n",
    "        return(np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4042de5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_depth_range(df, depth_col):\n",
    "    '''Function to get the depth range of the passed DataFrame.\n",
    "    Mandatory arguments:\n",
    "    :df: pandas DataFrame with concatenated depth-related data (the DataFrame is generated through \n",
    "       functions \"pick_up_depth_related_data\" and\n",
    "       \"check_whether_different_dataframes_depth_related_data_are_on_same_depths\")\n",
    "    :depth_col: name of the (renamed) depth column\n",
    "    Returns the minimum and maximum depth as a tuple (used for the LAS header)'''\n",
    "    try:\n",
    "        return(round(min(df[depth_col]),4), round(max(df[depth_col]),4))\n",
    "    except:\n",
    "        return(np.nan, np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5516289a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_LAS_well_section(las, df, tables, loca):\n",
    "    '''Function to fill fields in the \"well section\" (\"~W\") of the LAS file.\n",
    "    Mandatory arguments:\n",
    "    :las: the las object (generated through function \"create_and_fill_LAS_file\")\n",
    "    :df: pandas DataFrame with concatenated depth-related data (the DataFrame is generated through \n",
    "       functions \"pick_up_de, pth_related_data\" and\n",
    "       \"check_whether_different_dataframes_depth_related_data_are_on_same_depths\")\n",
    "    :tables: tables-section from AGS file\n",
    "    :loca: unique location name, used in the AGS file\n",
    "    Does not return anything, but changes the ~W section of the current LAS-object'''\n",
    "    las.well.STRT = get_depth_range(df, depth_col)[0]\n",
    "    las.well.STOP = get_depth_range(df, depth_col)[1]\n",
    "    las.well.STEP = get_sampling_rate(df, depth_col)\n",
    "    las.well.NULL = -999.2500000\n",
    "    las.well.COMP = tables['PROJ'].loc[2,'PROJ_CLNT']\n",
    "    las.well.WELL = loca\n",
    "    las.well.FLD = tables['PROJ'].loc[2,'PROJ_NAME']\n",
    "    las.well.LOC = tables['PROJ'].loc[2,'PROJ_LOC']    \n",
    "    las.well.PROV = ''\n",
    "    las.well.CNTY = ''\n",
    "    las.well.STAT = ''\n",
    "    las.well.CTRY = ''\n",
    "    las.well.SRVC = tables['PROJ'].loc[2,'PROJ_CONT']\n",
    "    las.well.DATE = datetime.today().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    las.well.API = 'NULL'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "291d1cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_LAS_curve_section(las, df, codes):\n",
    "    '''Function to fill the \"curve section\" (\"~C\") of the LAS file.\n",
    "    Mandatory arguments:\n",
    "    :las: the las object (generated through function \"create_and_fill_LAS_file\")\n",
    "    :df: pandas DataFrame with concatenated depth-related data (the DataFrame is generated through \n",
    "       functions \"pick_up_depth_related_data\" and\n",
    "       \"check_whether_different_dataframes_depth_related_data_are_on_same_depths\")\n",
    "    :codes: dictionary with all (depth-related) codes that should be looked for. The keys should\n",
    "       be the GROUP_HEADER codes (e.g. \"SCPT_QT\") that will be looked up; the values are\n",
    "       the descriptions\n",
    "    Does not return anything, but changes the ~C section of the current LAS-object'''\n",
    "    tmp = list()\n",
    "    for i, c in enumerate(df.columns):\n",
    "        tmp.append(find_depth_unit(tables, c))\n",
    "        if len(set(tmp))>1:\n",
    "            depth_unit = list(set(tmp))[0]\n",
    "        else:\n",
    "            depth_unit = max(set(tmp), key=tmp.count)\n",
    "\n",
    "    for i, c in enumerate(df.columns):\n",
    "        if i == 0:\n",
    "            las.add_curve(c, df[c], unit=depth_unit)\n",
    "        else:\n",
    "            unit = find_unit(tables, c)\n",
    "            descr = ''.join([d[1] for d in codes.items() if d[0]==c])\n",
    "            las.add_curve(c, df[c], descr=descr, unit=unit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9066cfae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_LAS_params_section(las, tables, loca, parameters):\n",
    "    '''Function to fill fields in the \"parameter section\" (\"~P\") of the LAS file.\n",
    "    Mandatory arguments:\n",
    "    :las: the las object (generated through function \"create_and_fill_LAS_file\")\n",
    "    :tables: tables-section from AGS file\n",
    "    :loca: unique location name, used in the AGS file\n",
    "    :parameters: dictionary with codes with meta-data to go into the\n",
    "       parameters section of the LAS-object\n",
    "    Does not return anything, but changes the ~P section of the current las-object.'''\n",
    "    for p in parameters.items():\n",
    "        try:\n",
    "            unit = find_unit(tables, p[0])\n",
    "            las.params[p[0]] = lasio.HeaderItem(mnemonic=p[0],\n",
    "                                                unit=unit,\n",
    "                                                value=tables['LOCA'].loc[(tables['LOCA']['LOCA_ID']==loca),p[0]].values[0],\n",
    "                                                descr=p[1])\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169191ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_LAS_other_section(las, loca, file):\n",
    "    '''Function to fill field in the \"other section\" (\"~O\") of the LAS file.\n",
    "    Mandatory arguments:\n",
    "    :las: the las object (generated through function \"create_and_fill_LAS_file\")\n",
    "    :loca: unique location name, used in the AGS file\n",
    "    :file: name of the AGS4 file\n",
    "    Does not return anything, but changes the ~O section of the current las-object.'''\n",
    "    try:\n",
    "        las.other = f'LAS file generated for data for location \"{loca}\". Original file: \"{file}\".'\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf56da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_depth_unit(tables, header):\n",
    "    '''Small function to get the units of depth.\n",
    "    Mandatory arguments:\n",
    "    :tables: tables-section from AGS file\n",
    "    :header: name of column'''\n",
    "    try:\n",
    "        unit = tables[header[:4]].loc[0,header[:4]+depth_suffix]\n",
    "    except:\n",
    "        unit = ''\n",
    "    return unit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb7386a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_unit(tables, header):\n",
    "    '''Small function to get the units of a parameter/variable.\n",
    "    Mandatory arguments:\n",
    "    :tables: tables-section from AGS file\n",
    "    :header: name of column'''\n",
    "    try:\n",
    "        unit = tables[header[:4]].loc[0,header]\n",
    "    except:\n",
    "        unit = ''\n",
    "    return unit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0500c9ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_conversion_log(export_folder, summary_processing):\n",
    "    '''Function writing the info regarding the conversion to a file \"conversion.log\" in the same LAS\n",
    "    export location. This log can be used to check locations/files that caused warnings or errors.\n",
    "    Mandatory input:\n",
    "    :export_folder: location of the exported LAS files (the log file will be written to the same location)\n",
    "    :summary processing: list with tuples (code, location, file) that will be written to the file\n",
    "    \"conversion.log\"\n",
    "    Returns nothing, but will put a file \"conversion.log\" in the same directory the LAS files were exported\n",
    "    to with info about the conversion job.'''\n",
    "    max_length_location_name = 0\n",
    "    max_length_file_name = 0\n",
    "    for i in range(2,-1,-1):\n",
    "        try:\n",
    "            if max([len(s[1]) for s in summary_processing if s[0] == i]) > max_length_location_name:\n",
    "                max_length_location_name = max([len(s[1]) for s in summary_processing if s[0] == i])\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            if max([len(s[2]) for s in summary_processing if s[0] == i]) > max_length_file_name:\n",
    "                max_length_file_name = max([len(s[2]) for s in summary_processing if s[0] == i])\n",
    "        except:\n",
    "            pass\n",
    "    headers = ['LOCATIONS IN FILE PROCESSED OK:\\n',\n",
    "               'LOCATIONS IN FILE WITHOUT ANY DEPTH-DATA (i.e no logs):\\n',\n",
    "               'LOCATIONS IN FILE THAT CAUSED AN ERROR:\\n']\n",
    "    \n",
    "    f = open(os.path.join(export_folder, 'conversion.log'), 'w')\n",
    "    for i in range(2,-1,-1):\n",
    "        f.write(headers[i])\n",
    "        f.write('-'*(max_length_location_name+11+max_length_file_name)+'\\n')\n",
    "        for c in [s for s in summary_processing if s[0] == i]:\n",
    "            f.write(f'{c[1]:<{max_length_location_name}} in file   {c[2]:<{max_length_file_name}}\\n')\n",
    "        f.write('\\n\\n')\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d1a2890",
   "metadata": {},
   "source": [
    "### Find all AGS files in a location (use find_files function defined earlier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c2bd73",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = os.path.join(r'C:\\Users\\harbr\\OneDrive - Equinor\\petrophysical mindboggles\\AGS_files\\AGS4_files')\n",
    "extension = '.AGS'\n",
    "\n",
    "ags4_files = find_files(folder, recursive=True, extension=extension, case_sensitive=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d03b64ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "ags4_files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28614266",
   "metadata": {},
   "source": [
    "## Convert all locations in all AGS files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c155b48d",
   "metadata": {},
   "outputs": [],
   "source": [
    "export_folder = os.path.join(r'C:\\Users\\harbr\\OneDrive - Equinor\\petrophysical mindboggles\\AGS_files\\output')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a0b3d3d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "summary_processing = list()\n",
    "for file in ags4_files:\n",
    "    print(f'...working on file {file}...')\n",
    "    tables, headers = AGS4.AGS4_to_dataframe(file) # extract tables and headers for current AGS file\n",
    "    locations = tables['LOCA'].drop([0,1]).reset_index(drop=True)['LOCA_ID'].unique() # find all locations in current AGS file\n",
    "    \n",
    "    for loca in locations:\n",
    "        print(f'   ...converting location {loca}...')\n",
    "        process_code, location, ags_filename = convert_data_for_single_location(loca, file, tables)\n",
    "        summary_processing.append((process_code, location, ags_filename))\n",
    "write_conversion_log(export_folder, summary_processing)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
